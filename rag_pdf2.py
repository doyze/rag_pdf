import base64
import gradio as gr
import os
import shutil
import pymupdf as fitz  # PyMuPDF
import numpy as np
from PIL import Image
import io
import qdrant_client
from qdrant_client import QdrantClient, models
from sentence_transformers import SentenceTransformer
from pythainlp.tokenize import word_tokenize
from transformers import MT5Tokenizer, MT5ForConditionalGeneration

import torch
import ollama
import uuid
import shortuuid
import logging
import re
from openai import OpenAI
import google.generativeai as genai
import google.api_core.exceptions
from dotenv import load_dotenv

from typing import List, Dict, Tuple

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Image folder
TEMP_IMG="./data/images"
TEMP_VECTOR="./data/chromadb"

summarize = ""

def get_available_models():
    """
    Dynamically fetches available models from Ollama and Google Gemini.
    """
    models = []
    # Fetch Ollama models
    try:
        ollama_models = ollama.list()
        # Use .get() for safer access to the 'name' key
        models.extend([model.get('name') for model in ollama_models.get('models', []) if model.get('name')])
    except Exception as e:
        logger.warning(f"Could not fetch Ollama models: {e}")

    # Fetch Gemini models
    try:
        if GEMINI_API_KEY:
            for m in genai.list_models():
                if 'generateContent' in m.supported_generation_methods:
                    models.append(m.name.replace("models/", "")) # Clean up the name
    except Exception as e:
        logger.warning(f"Could not fetch Gemini models: {e}")
    
    # Fallback to a default list if fetching fails
    if not models:
        logger.warning("Could not dynamically fetch models, using fallback list.")
        models = ["pdf-gemma", "pdf-qwen", "pdf-llama", "gemini-1.5-flash", "gemini-1.5-pro-latest"]
        
    return sorted(list(set(models))) # Return sorted unique list

# Load environment variables from .env file
load_dotenv()

# Google Gemini API Key
GEMINI_API_KEY = os.getenv("GOOGLE_API_KEY")
if GEMINI_API_KEY:
    genai.configure(api_key=GEMINI_API_KEY)
else:
    logger.warning("GOOGLE_API_KEY not found in .env file. Gemini models will not be available.")

AVAILABLE_MODELS = get_available_models()

# LM Studio Client
lm_studio_client = OpenAI(base_url="http://127.0.0.1:1234/v1", api_key="not-needed")


# ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
logging.info(f"Using device: {device}")

# ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏• embedding
# SentenceTransformer ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏•‡∏≤‡∏¢‡∏†‡∏≤‡∏©‡∏≤ (‡πÄ‡∏ô‡πâ‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢)
sentence_model = SentenceTransformer('intfloat/multilingual-e5-large', device=device)

# Initialize Qdrant client
qdrant_client = QdrantClient(url="http://localhost:6333")
collection_name = "pdf_data"

# Create collection if it doesn't exist
try:
    if not qdrant_client.collection_exists(collection_name=collection_name):
        qdrant_client.create_collection(
            collection_name=collection_name,
            vectors_config=models.VectorParams(size=sentence_model.get_sentence_embedding_dimension(), distance=models.Distance.COSINE),
        )
except Exception as e:
    logging.error(f"Error creating collection: {e}")

# Create directory for storing images
os.makedirs(TEMP_IMG, exist_ok=True)

sum_tokenizer = MT5Tokenizer.from_pretrained('StelleX/mt5-base-thaisum-text-summarization')
sum_model = MT5ForConditionalGeneration.from_pretrained('StelleX/mt5-base-thaisum-text-summarization').to(device)

def summarize_content(content: str) -> str:
    """
        ‡∏™‡∏£‡∏∏‡∏õ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤ 
    """
    logging.info("%%%%%%%%%%%%%% SUMMARY %%%%%%%%%%%%%%%%%%%%%")    
       
    input_ = sum_tokenizer(content, truncation=True, max_length=1024, return_tensors="pt")
    with torch.no_grad():
        preds = sum_model.generate(
            input_['input_ids'].to(device),
            num_beams=15,
            num_return_sequences=1,
            no_repeat_ngram_size=1,
            remove_invalid_values=True,
            max_length=250
        )

    summary = sum_tokenizer.decode(preds[0], skip_special_tokens=True)

    logging.info(f" summary: {summary}.")
    logging.info("%%%%%%%%%%%%%% SUMMARY %%%%%%%%%%%%%%%%%%%%%")
    return summary

# ‡πÅ‡∏¢‡∏Å‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤, ‡∏£‡∏π‡∏õ ‡∏≠‡∏≠‡∏Å‡∏à‡∏≤‡∏Å PDF
def ocr_with_typhoon(image_path: str) -> str:
    """
    ‡πÉ‡∏ä‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏• typhoon-ocr-7b ‡∏ú‡πà‡∏≤‡∏ô LM Studio ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≤‡∏Å‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û
    """
    try:
        logging.info(f"Performing OCR on {image_path}")
        with open(image_path, "rb") as img_file:
            base64_image = base64.b64encode(img_file.read()).decode('utf-8')

        response = lm_studio_client.chat.completions.create(
            model="local-model/typhoon-ocr-7b",
            messages=[
                {
                    "role": "user",
                    "content": [
                        {"type": "text", "text": "Extract text from this image."},
                        {
                            "type": "image_url",
                            "image_url": {
                                "url": f"data:image/jpeg;base64,{base64_image}"
                            },
                        },
                    ],
                }
            ],
            max_tokens=2048,
            timeout=60.0,
        )
        ocr_text = response.choices[0].message.content
        logging.info(f"OCR result: {ocr_text[:100]}...")
        return ocr_text
    except Exception as e:
        logger.error(f"Error during OCR with Typhoon: {e}", exc_info=True)
        return ""

def extract_pdf_content(pdf_path: str) -> List[Dict]:
    """
    ‡πÅ‡∏¢‡∏Å‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏•‡∏∞‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡∏à‡∏≤‡∏Å PDF ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ PyMuPDF ‡πÅ‡∏•‡∏∞ Typhoon OCR
    """
    try:
        doc = fitz.open(pdf_path)
        content_chunks = []
        all_text = []

        for page_num in range(len(doc)):
            page = doc[page_num]
            # Extract text using PyMuPDF
            text = page.get_text("text").strip()
            if not text:
                text = f"‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÉ‡∏ô‡∏´‡∏ô‡πâ‡∏≤ {page_num + 1}"
            
            logging.info("################# Text data ##################")
            chunk_data = {"text": f"‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡∏´‡∏ô‡πâ‡∏≤ {page_num + 1} : {text}", "images": []}
            
            # Extract images
            image_list = page.get_images(full=True)
            logging.info("################# images list ##################")
            for img_index, img in enumerate(image_list):
                xref = img[0]
                base_image = doc.extract_image(xref)
                image_bytes = base_image["image"]
                image_ext = base_image["ext"]
                
                try:
                    image = Image.open(io.BytesIO(image_bytes))
                    if image.mode != "RGB":
                        image = image.convert("RGB")
                    
                    img_id = f"pic_{page_num + 1}_{img_index + 1}"
                    img_path = f"{TEMP_IMG}/{img_id}.{image_ext}"
                    image.save(img_path, format=image_ext.upper())

                    # Perform OCR with Typhoon
                    ocr_text = ocr_with_typhoon(img_path)
                    all_text.append(f"{ocr_text}\n\n\n")

                    img_desc = f"‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û ‡∏à‡∏≤‡∏Å‡∏´‡∏ô‡πâ‡∏≤ {page_num + 1} ‡∏Ç‡∏≠‡∏á ‡∏£‡∏π‡∏õ‡∏ó‡∏µ‡πà {img_index + 1}, OCR Text: {ocr_text[:80]}..."
                    chunk_data["text"] += f"\n[‡∏†‡∏≤‡∏û: {img_id}.{image_ext} - {ocr_text}]"
                    chunk_data["images"].append({
                        "data": image,
                        "path": img_path,
                        "description": img_desc
                    })
                except Exception as e:
                    logger.warning(f"‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡∏ó‡∏µ‡πà‡∏´‡∏ô‡πâ‡∏≤ {page_num + 1}, ‡∏£‡∏π‡∏õ‡∏ó‡∏µ‡πà {img_index + 1}: {e}")
            
            all_text.append(f"{text}\n\n\n")
            if chunk_data["text"]:
                content_chunks.append(chunk_data)
        
        if not any(chunk["images"] for chunk in content_chunks):
            logger.warning("‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡πÉ‡∏ô PDF: %s", pdf_path)
        
        doc.close()
        content_text = "".join(all_text)
        # ‡∏ï‡∏±‡∏î‡∏Ñ‡∏≥‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢
        thaitoken_text = preprocess_thai_text(content_text) if any(ord(c) >= 0x0E00 and ord(c) <= 0x0E7F for c in content_text) else content_text
        print("################################")
        print(f"{thaitoken_text}")
        print("################################")
        global summarize
        summarize = summarize_content(thaitoken_text)
        return content_chunks
    except Exception as e:
        logger.error("‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÅ‡∏¢‡∏Å PDF: %s", str(e))
        raise

# ‡∏ï‡∏±‡∏î‡∏Ñ‡∏≥‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ 
def preprocess_thai_text(text: str) -> str:
    """
    ‡∏ï‡∏±‡∏î‡∏Ñ‡∏≥‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡∏î‡πâ‡∏ß‡∏¢ pythainlp ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°

    Args:
        text (str): ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢

    Returns:
        str: ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏ï‡∏±‡∏î‡∏Ñ‡∏≥‡πÅ‡∏•‡πâ‡∏ß
    """
    return " ".join(word_tokenize(text, engine="newmm"))


def embed_text(text: str) -> np.ndarray:
    """
    ‡∏™‡∏£‡πâ‡∏≤‡∏á embedding ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ SentenceTransformer 

    Args:
        text (str): ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á embedding        

    Returns:
        np.ndarray: Embedding vector ‡∏ó‡∏µ‡πà‡∏£‡∏ß‡∏°‡∏à‡∏≤‡∏Å‡∏´‡∏•‡∏≤‡∏¢‡πÇ‡∏°‡πÄ‡∏î‡∏•
    """
    logging.info("-------------- start embed text  -------------------")
    
    # ‡∏ï‡∏±‡∏î‡∏Ñ‡∏≥‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢
    processed_text = preprocess_thai_text(text) if any(ord(c) >= 0x0E00 and ord(c) <= 0x0E7F for c in text) else text
    
    # ‡∏™‡∏£‡πâ‡∏≤‡∏á embedding ‡∏î‡πâ‡∏ß‡∏¢ SentenceTransformer
    sentence_embedding = sentence_model.encode(processed_text, normalize_embeddings=True, device=device)    
        
    return sentence_embedding

def store_in_qdrant(content_chunks: List[Dict], pdf_name: str):
    """
    ‡πÄ‡∏Å‡πá‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏•‡∏∞‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡πÉ‡∏ô Qdrant ‡∏û‡∏£‡πâ‡∏≠‡∏° embedding
    """
    logging.info("##### Start store in qdrant #########")
    points = []
    for chunk in content_chunks:
        text = chunk["text"]
        images = chunk["images"]
        text_embedding = embed_text(text)
        
        points.append(
            models.PointStruct(
                id=str(uuid.uuid4()),
                vector=text_embedding.tolist(),
                payload={"type": "text", "source": pdf_name, "text": text}
            )
        )

        for img in images:
            img_path = img["path"]
            points.append(
                models.PointStruct(
                    id=str(uuid.uuid4()),
                    vector=text_embedding.tolist(),
                    payload={"type": "image", "source": pdf_name, "image_path": img_path, "text": text}
                )
            )
    
    qdrant_client.upsert(collection_name=collection_name, points=points, wait=True)

def process_pdf_upload(pdf_file):
    """
    ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Å‡∏≤‡∏£‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• PDF
    """
    try:
        if pdf_file is None:
            return "‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå PDF ‡∏Å‡πà‡∏≠‡∏ô", "", None
        
        pdf_path = pdf_file.name
        pdf_name = os.path.basename(pdf_path)
        logging.info(f"#### Start processing {pdf_name} ####")
        
        content_chunks = extract_pdf_content(pdf_path)
        if not content_chunks:
            return f"‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÅ‡∏¢‡∏Å‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏à‡∏≤‡∏Å {pdf_name} ‡πÑ‡∏î‡πâ", "", None

        logging.info("#### Storing content in vector db ####")
        store_in_qdrant(content_chunks, pdf_name)
        
        logging.info(f"#### Finished processing {pdf_name} ####")
        
        indexed_files_text = "\n".join(get_indexed_files())
        return f"‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÅ‡∏•‡∏∞‡∏à‡∏±‡∏î‡πÄ‡∏Å‡πá‡∏ö {pdf_name} ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à", indexed_files_text, None
    except Exception as e:
        logger.error(f"‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• PDF: {e}", exc_info=True)
        return f"‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• PDF: {str(e)}", "", None

def clear_vector_db():
    try:
        if qdrant_client.collection_exists(collection_name=collection_name):
            qdrant_client.delete_collection(collection_name=collection_name)
        qdrant_client.create_collection(
            collection_name=collection_name,
            vectors_config=models.VectorParams(size=sentence_model.get_sentence_embedding_dimension(), distance=models.Distance.COSINE),
        )
    except Exception as e:
        logging.error(f"Error clearing Qdrant collection: {e}")
        raise e

def clear_all_data():
    """
    ‡∏•‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô vector database ‡πÅ‡∏•‡∏∞‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏ô‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå images
    """
    try:
        clear_vector_db()
        if os.path.exists(TEMP_IMG):
            shutil.rmtree(TEMP_IMG)
            os.makedirs(TEMP_IMG, exist_ok=True)
        
        return "‡∏•‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô vector database ‡πÅ‡∏•‡∏∞‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå images ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à", None, ""
    except Exception as e:
        return f"‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏•‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•: {str(e)}", None, ""

def get_indexed_files():
    """
    ‡∏î‡∏∂‡∏á‡∏£‡∏≤‡∏¢‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏à‡∏±‡∏î‡πÄ‡∏Å‡πá‡∏ö‡πÉ‡∏ô Qdrant
    """
    try:
        # This is a simplified way to get distinct sources.
        # For very large collections, a more efficient approach might be needed.
        response, _ = qdrant_client.scroll(
            collection_name=collection_name,
            limit=10000,  # Adjust limit as needed
            with_payload=["source"],
            with_vectors=False
        )
        sources = {point.payload['source'] for point in response}
        return list(sources)
    except Exception as e:
        logger.error(f"Error fetching indexed files: {e}")
        return []


def query_rag(question: str,  chat_llm: str = "pdf-qwen"):
    """
    ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÉ‡∏ô‡∏£‡∏∞‡∏ö‡∏ö RAG ‡πÅ‡∏•‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡πÅ‡∏ö‡∏ö streaming ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ Ollama
    """
    logging.info(f"####  RAG get Question #### ")
    question_embedding = embed_text(question)
    
    results=[]
    # ‡πÄ‡∏ä‡πá‡∏Ñ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å ‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£ ‡∏î‡∏∂‡∏á‡∏°‡∏≤ 3 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£   ##  Retrival
    max_result = 3
    if "‡∏Å‡∏µ‡πà" in question:
        max_result = 5
    
    if "‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î" in question:
        max_result = 10

    if "‡∏ö‡πâ‡∏≤‡∏á" in question:
        max_result = 5

    results = qdrant_client.search(
        collection_name=collection_name,
        query_vector=question_embedding.tolist(),
        limit=max_result
    )
    logging.info(f"##### results from vector: { results }")
    context_texts = []
    image_paths = []

    for result in results:
        doc = result.payload['text']
        metadata = result.payload
        context_texts.append(doc)
        logging.info(doc)
        logging.info(f"metadata: {metadata}")
        # Regex pattern ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤ [img: ‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå.jpeg]
        pattern = r"pic_(\d+)_(\d+)\.jpeg"

        # ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏ó‡∏∏‡∏Å‡∏£‡∏π‡∏õ ‡πÅ‡∏ö‡∏ö‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö ‡∏™‡πà‡∏á‡πÄ‡∏Ç‡πâ‡∏≤‡∏°‡∏≤
        imgs = re.findall(pattern, doc)
        print("----------IIIII------------")
        print(imgs)
        print("----------IIIII------------")
        if imgs:
            image_paths.append(imgs)
            logging.info(f"img: {imgs}")

        print("---------------------------")
        if metadata:
            if metadata["type"] == "image":
                logging.info(f"image_path : { metadata['image_path']}")
                image_paths.append(metadata['image_path'])
    


    context = "\n".join(context_texts)
    ##  Augmented
    logging.info("############## Begin Augmented prompt #################")
    prompt = f"""‡∏à‡∏≤‡∏Å‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏ï‡πà‡∏≠‡πÑ‡∏õ‡∏ô‡∏µ‡πâ ‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°: {question}

    ‡∏ö‡∏£‡∏¥‡∏ö‡∏ó: 
        {summarize}

        {context}

    ‡πÉ‡∏´‡πâ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏ó‡∏µ‡πà‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô‡πÅ‡∏•‡∏∞‡∏Å‡∏£‡∏∞‡∏ä‡∏±‡∏ö‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ ‡∏´‡∏≤‡∏Å‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏°‡∏µ‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û ‡πÉ‡∏´‡πâ‡∏£‡∏∞‡∏ö‡∏∏‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏ô‡∏ß‡∏á‡πÄ‡∏•‡πá‡∏ö‡πÄ‡∏´‡∏•‡∏µ‡πà‡∏¢‡∏°‡πÅ‡∏ö‡∏ö‡∏ô‡∏µ‡πâ: [‡∏†‡∏≤‡∏û: ‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå] """ 
    
    logging.info(f"promt: {prompt}")
    logging.info("##############  End Augmented prompt #################")

    logging.info("+++++++++++++  Send prompt To LLM  ++++++++++++++++++")
    ## Generation  ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Å‡∏≤‡∏£‡∏ï‡∏≠‡∏ö chat
    if chat_llm.startswith("gemini"):
        gemini_model = genai.GenerativeModel(chat_llm)
        stream = gemini_model.generate_content(prompt, stream=True)
    else:
        stream = ollama.chat(
            model=chat_llm,
            messages=[{"role": "user", "content": prompt}],
            stream=True
        )
    
    return stream

def user(user_message: str, history: List[Dict]) -> Tuple[str, List[Dict]]:
    """
    ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ input ‡∏Ç‡∏≠‡∏á‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡πÅ‡∏•‡∏∞‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏•‡∏á‡πÉ‡∏ô‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡πÅ‡∏ä‡∏ó
    """
    return "", history + [{"role": "user", "content": user_message}]

def chatbot_interface(history: List[Dict], llm_model: str):
    """
    ‡∏≠‡∏¥‡∏ô‡πÄ‡∏ó‡∏≠‡∏£‡πå‡πÄ‡∏ü‡∏ã‡πÅ‡∏ä‡∏ó‡∏ö‡∏≠‡∏ó‡πÅ‡∏ö‡∏ö streaming
    """
    user_message = history[-1]["content"]
    
    stream= query_rag(user_message, chat_llm=llm_model)

    history.append({"role": "assistant", "content": ""})
    full_answer=""
    """
    ‡∏™‡πà‡∏ß‡∏ô‡∏Ç‡∏≠‡∏á‡∏Å‡∏≤‡∏£ ‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°
    """
    try:
        for chunk in stream:
            if llm_model.startswith("gemini"):
                content = chunk.text
            else:
                content = chunk["message"]["content"]
            
            full_answer += content
            history[-1]["content"] += content
            yield history
    except google.api_core.exceptions.ResourceExhausted as e:
        logger.error(f"Gemini API quota exceeded: {e}", exc_info=True)
        error_message = "‡∏Ç‡∏≠‡∏≠‡∏†‡∏±‡∏¢, ‡∏Ñ‡∏∏‡∏ì‡πÉ‡∏ä‡πâ‡πÇ‡∏Ñ‡∏ß‡∏ï‡πâ‡∏≤‡∏ü‡∏£‡∏µ‡∏Ç‡∏≠‡∏á Gemini API ‡πÄ‡∏Å‡∏¥‡∏ô‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÅ‡∏•‡πâ‡∏ß ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏ú‡∏ô‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì"
        history[-1]["content"] = error_message
        yield history
    except google.api_core.exceptions.ServiceUnavailable as e:
        logger.error(f"Gemini API service unavailable: {e}", exc_info=True)
        error_message = "‡∏Ç‡∏≠‡∏≠‡∏†‡∏±‡∏¢, ‡πÇ‡∏°‡πÄ‡∏î‡∏• Gemini ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏´‡∏ô‡∏±‡∏Å‡πÉ‡∏ô‡∏Ç‡∏ì‡∏∞‡∏ô‡∏µ‡πâ ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏•‡∏≠‡∏á‡πÉ‡∏´‡∏°‡πà‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÉ‡∏ô‡∏†‡∏≤‡∏¢‡∏´‡∏•‡∏±‡∏á"
        history[-1]["content"] = error_message
        yield history
    except Exception as e:
        logger.error(f"Error during response generation: {e}", exc_info=True)
        error_message = f"‡∏Ç‡∏≠‡∏≠‡∏†‡∏±‡∏¢, ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö: {e}"
        history[-1]["content"] = error_message
        yield history
    

    """
    ‡∏™‡πà‡∏ß‡∏ô‡∏Ç‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏î‡∏∂‡∏á‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û ‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏°‡∏≤‡πÅ‡∏™‡∏î‡∏á ‡πÇ‡∏î‡∏¢‡∏î‡∏∂‡∏á‡∏à‡∏≤‡∏Å ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏î‡πâ‡∏≤‡∏ô‡∏ö‡∏ô 
    """

    # ‡πÉ‡∏ä‡πâ regex ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏∂‡∏á‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô [‡∏†‡∏≤‡∏û: ...] 
    print(full_answer)
    pattern1 = r"\[(?:‡∏†‡∏≤‡∏û:\s*)?(pic_\w+[-_]?\w*\.(?:jpe?g|png))\]"
    pattern2 = r"(pic_\w+[-_]?\w*\.(?:jpe?g|png))"
    # ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏ó‡∏∏‡∏Å‡∏£‡∏π‡∏õ ‡πÅ‡∏ö‡∏ö‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö ‡∏™‡πà‡∏á‡πÄ‡∏Ç‡πâ‡∏≤‡∏°‡∏≤
    
    print("----------PPPP------------")       
    image_list = re.findall(pattern1, full_answer)
    print(image_list)
    if (len(image_list)==0):
        image_list = re.findall(pattern2, full_answer)
    print("----------xxxx------------")  
    # ‡∏î‡∏∂‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏£‡∏π‡∏õ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏ã‡πâ‡∏≥‡∏Å‡∏±‡∏ô
    image_list_uniq = list(dict.fromkeys(image_list))  
    if image_list_uniq:
        history[-1]["content"] += "\n\n**‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á:**"
        yield history
        # ‡∏î‡∏∂‡∏á‡∏£‡∏π‡∏õ‡∏°‡∏≤‡πÅ‡∏™‡∏î‡∏á
        for img in image_list_uniq:
            img_path = f"{TEMP_IMG}/{img}"
            logger.info(f"Displaying image: {img_path}")
            if os.path.exists(img_path):
                try:
                    with open(img_path, "rb") as image_file:
                        encoded_string = base64.b64encode(image_file.read()).decode()
                    
                    mime_type = f"image/{os.path.splitext(img)[1][1:]}"
                    image_response = f"![{img}](data:{mime_type};base64,{encoded_string})"
                    
                    history.append(
                        {
                            "role": "assistant",
                            "content": image_response,
                        }
                    )
                    yield history
                except Exception as e:
                    logger.error(f"Error displaying image {img_path}: {e}")



# Gradio interface
with gr.Blocks() as demo:
    logo="https://camo.githubusercontent.com/9433204b08afdc976c2e4f5a4ba0d81f8877b585cc11206e2969326d25c41657/68747470733a2f2f63646e2e6a7364656c6976722e6e65742f67682f6e61726f6e67736b6d6c2f68746d6c352d6c6561726e406c61746573742f6173736574732f696d67732f546c697665636f64654c6f676f2d3435302e77656270"
    gr.Markdown(f"""<h3 style='display: flex; align-items: center; gap: 15px; padding: 10px; margin: 0;'>
        <img alt='T-LIVE-CODE' src='{logo}' style='height: 100px;' >
        <span style='font-size: 1.5em;'>‡πÅ‡∏ä‡∏ó‡∏ö‡∏≠‡∏ó PDF: RAG</span></h3>""")

    with gr.Tab("‡πÅ‡∏≠‡∏î‡∏°‡∏¥‡∏ô - ‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î PDF"):
        with gr.Row():
            with gr.Column(scale=1):
                pdf_input = gr.File(label="‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå PDF")
                upload_button = gr.Button("‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• PDF")
                clear_button = gr.Button("‡∏•‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•")
                upload_output = gr.Textbox(label="‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏Å‡∏≤‡∏£‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î")
            with gr.Column(scale=1):
                indexed_files_display = gr.Textbox(
                    label="‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏ô‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•",
                    lines=10,
                    interactive=False,
                    value="\n".join(get_indexed_files())
                )

        upload_button.click(
            fn=process_pdf_upload,
            inputs=pdf_input,
            outputs=[upload_output, indexed_files_display, pdf_input]
        )
        clear_button.click(
            fn=clear_all_data,
            inputs=None,
            outputs=[upload_output, pdf_input, indexed_files_display],
            queue=False
        )
    
    with gr.Tab("‡πÅ‡∏ä‡∏ó"):
        # Choice ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å Model
        model_selector = gr.Dropdown(
            choices=AVAILABLE_MODELS,
            value="gemini-1.5-flash",
            label="‡πÄ‡∏•‡∏∑‡∏≠‡∏Å LLM Model"
        )
        selected_model = gr.State(value="pdf-gemma")  # ‡πÄ‡∏Å‡πá‡∏ö‡πÑ‡∏ß‡πâ‡πÉ‡∏ô state
        model_selector.change(fn=lambda x: x, inputs=model_selector, outputs=selected_model)
        # Chat Bot
        chatbot = gr.Chatbot(type="messages")
        with gr.Row():
            msg = gr.Textbox(label="‡∏ñ‡∏≤‡∏°‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö PDF", elem_id="chat_msg", scale=9)
            clear_chat = gr.Button("‡∏•‡πâ‡∏≤‡∏á", scale=1)

        speech_to_text_html = """
        <div style="display:flex; gap:10px; align-items:center;">
            <button id="record_button_custom" title="‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÄ‡∏™‡∏µ‡∏¢‡∏á" style="font-size: 1.5em; background-color: #007bff; color: white; border: none; cursor: pointer; transition: background-color 0.3s; border-radius: 5px; width: 50px; height: 50px;">üé§</button>
            <span id="speech_status_custom" style="font-style: italic; color: #6c757d;"></span>
        </div>
        <script>
        function setup_speech_recognition() {
            const button = document.getElementById('record_button_custom');
            const status_label = document.getElementById('speech_status_custom');
            const msg_input = document.querySelector('#chat_msg textarea');

            if (!('webkitSpeechRecognition' in window)) {
                status_label.innerText = "API ‡πÑ‡∏°‡πà‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö";
                button.disabled = true;
                return;
            }

            const recognition = new webkitSpeechRecognition();
            recognition.lang = 'th-TH';
            recognition.continuous = true;
            recognition.interimResults = true;

            recognition.onstart = function() {
                status_label.innerText = "‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ü‡∏±‡∏á...";
                button.style.backgroundColor = '#dc3545'; // Red
                window.recognition_active = true;
            };

            recognition.onresult = function(event) {
                let final_transcript = '';
                for (let i = event.resultIndex; i < event.results.length; ++i) {
                    if (event.results[i].isFinal) {
                        final_transcript += event.results[i][0].transcript;
                    }
                }
                if (final_transcript) {
                    msg_input.value += final_transcript.trim() + ' ';
                    const input_event = new Event('input', { bubbles: true });
                    msg_input.dispatchEvent(input_event);
                }
            };

            recognition.onerror = function(event) {
                status_label.innerText = "‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: " + event.error;
                button.style.backgroundColor = '#007bff'; // Blue
                window.recognition_active = false;
            };

            recognition.onend = function() {
                status_label.innerText = "";
                button.style.backgroundColor = '#007bff'; // Blue
                window.recognition_active = false;
            };

            button.onclick = function() {
                if (window.recognition_active) {
                    recognition.stop();
                } else {
                    navigator.mediaDevices.getUserMedia({ audio: true })
                        .then(function(stream) {
                            window.localStream = stream;
                            recognition.start();
                        })
                        .catch(function(err) {
                            status_label.innerText = "‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö‡∏≠‡∏ô‡∏∏‡∏ç‡∏≤‡∏ï‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ‡πÑ‡∏°‡πÇ‡∏Ñ‡∏£‡πÇ‡∏ü‡∏ô";
                            console.error('Error getting media stream', err);
                        });
                }
            };
        }

        function runWhenReady() {
            if (document.getElementById('record_button_custom')) {
                setup_speech_recognition();
            } else {
                setTimeout(runWhenReady, 100);
            }
        }

        runWhenReady();
        </script>
        """
        gr.HTML(speech_to_text_html)
        
        # Submit function 
        msg.submit(
            fn=user,
            inputs=[msg, chatbot],
            outputs=[msg, chatbot],
            queue=False
        ).then(
            fn=chatbot_interface,
            inputs=[chatbot, selected_model],
            outputs=chatbot
        )
        clear_chat.click(lambda: [], None, chatbot, queue=False)

if __name__ == "__main__":
    # The application will no longer clear data on startup.
    # Data will only be cleared when the "‡∏•‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•" button is pressed.
    demo.launch()
